
import sys, os
from pathlib import PurePath
file_dir   = PurePath(os.path.dirname(os.path.realpath(__file__)))
src_dir    = str(file_dir.parents[0]) # ./src
root_dir   = str(file_dir.parents[1]) # ./
sys.path.insert(0, src_dir)

import os
import pandas as pd
from sklearn.model_selection import KFold
import math
import numpy as np
import time
import argparse

from utilities.evaluation_functions import get_accuracy, make_predictions_dict, make_predictions_dict_inparallel,aggregated_CFformula, get_utility_coverage, get_user_space_coverage,\
						get_precision_and_recall, aggregated_CFformula, unweighted_CFformula, vanilla_CFformula, vanilla_CFformula_with_sim_to_sender, vanilla_CFformula_with_mean_sims,\
						get_accuracy_overall, get_overall_coverage, get_per_user_coverage, count_entries, count_data_exchanges, make_mean_dict, drop_rows_with_nans,\
						remove_nan_rows_from_predictions_dict, make_baseline_predictions_dict, log_result_table

from utilities.util import make_train_test_split, make_train_test_split_per_user, load_ratings_df, unmake_snapshot_info_string

# this module is meant to evaluate decentralized algorithms on the basis of a directory full of personal snapshot_dfs
# however, as an input, we only need
# 1. The same input dataset
# 2. The number of splits (n_splits for KFolds)
# 3. The same traintest_seed integer for the test_split (KFolds)
# 4. The fold number
# 5. The path to a directory holding the necessary snapshots





def run_evaluation(datapath, n_splits, traintest_seed, foldNr, snapshot_dirs, drop_nans = False, CFformula = None, k_min = 1, k_max = 50, \
		by = "sim_to_sender", round_int = 4, experiment_name = "test", round_i = 4, drop_percentile = None, timestamp_mins = None, mean_centering = False, ttsplit_per_user = False):
	""" Evaluation function that calculates MSE, user-space-coverage, Precision@10, Recall@10, F1@10, and RMSE/user-space-coverage for a list of snapshot_dirs.
	Input:
		- datapath				: <str> path to the data (user-item information) to use
		- n_splits				: <int> number of splits to perform on (this should be equal as in X) such that the test set is the complement of the train set used in the snapshot calculation
		- traintest_seed			: <int> traintest_seed for splitting the test and train sets, this should match the random state used for the calculation of the snapshots
		- foldNr				: <int> number of the fold. the foldNr counts from 0,.., n_splits - 1
		- snapshot_dirs	: <str> path of the directory holding all snapshot-files (.csv files) with user ID names (+'.csv')
		- drop_nans				: <bool> True if nans are to be dropped in the test_table (true and estimated values of the test set)
		- CFformula				: <function-name> to the function to use for evaluation of the rating estimates (have to be able to act on the snapshot-files) [e.g. vanilla_CFformula for
								  snapshots generated by DecCF, and aggregated_CFformula for snaphots generated by DecAggCF]
		- k_min					: <int> only derive an estimated rating when a least k_min neighbor ratings are available
		- k_max					: <int> only derive an estimated rating on the k_max most similar neighbors' ratings
		- by 					: <str> only relevant if k_max is given, then by defines the column name (e.g. 'sim_to_sender' according to which the k_max ratings are to be used for rating estimation
		- round_int				: <int>, number of decimals to round to [Note that this is only used for the presentation of the results]
		- experiment_name		: <str> name for the rows in the final output table, here by default we increment the row name by ":t=(i*5)", where i is the index
									of the snapshot_dir in the list snapshot_dirs
		- round_i				: <int> number of decimals to round the result entries to [only relevant for the display of the results]
		- drop_percentile		: <float> drop all entries of a snapshot_df that are within the drop_percentile of <by>
									e.g. by = "sim_to_sender" and drop_percentile = 0.9 --> all entries of snapshot_df that are within the
									lower 90% of the "sim_to_sender" distribution. Afterwards estimate ratings on the rest
		- timestamp_mins		: <list> of <int> corresponding to snapshot_dirs. timestamp_thresholds before which entries are dropped (not considered for
						  rating estimation

	# 1. Read in the ratings .csv
	##############################
		FORMAT
			# read the data in the form of a table with columns
			# 'userId'	'itemId'	'rating'	'timestamp'; where timestamp will not be used
			#

	# 2. Split train/test sets
	#############################

	# 3. Calculate metrics for all snapshots_output_paths
	#############################

	# 3.1 For snapshots_output_path in snapshots_output_paths
		# 3.1.1 Create <predictions_dict>
		# 3.1.2 Calculate metrics on <predictions_dict>
		# 3.1.3 Assemble output dataframe (specify metrics to show in the output table)


	######################################
	# Examples:                          #
	######################################
	{userId: array([[itemId, estimated_rating],...])}

	1. <predictions_dict> on the "./data/test" dataset with nans.
		>>> predictions_dict_with_nans
		{'1': array([[2. , 3.5], [3. , 3. ]]),
		 '2': array([[ 2., nan]]),
		 '3': array([[1.        , 4.33333333]]),
		 '4': array([[ 3., nan]]),
		 '6': array([[5., 3.]]),
		 '8': array([[ 2., nan]])}

	2. <predictions_dict> on the ""./data/test" dataset without nans.
		>>> predictions_dict_with_nans
		{'1': array([[2. , 3.5], [3. , 3. ]]),
		 '3': array([[1.        , 4.33333333]]),
		 '6': array([[5., 3.]])}


	Output:
		- output_table			: pandas.DataFrame with rows of the snapshots_output_paths, and columns of the evaluation criteria
	"""
	#################################
	# 1. LOAD RATINGS               #
	#################################
	rating_df = load_ratings_df(datapath, userId_label = "userId", itemId_label = "itemId", rating_label = "rating", timestamp_label = "timestamp")

	if CFformula is None:
		CFformula_str = "none"
	elif CFformula == aggregated_CFformula:
		CFformula_str = "aggregated_CFformula"
	elif CFformula == unweighted_CFformula:
		CFformula_str = "unweighted_CFformula"
	elif CFformula == vanilla_CFformula:
		CFformula_str = "vanilla_CFformula"
	elif CFformula == vanilla_CFformula_with_sim_to_sender:
		CFformula_str = "vanilla_CFformula_with_sim_to_sender"
	elif CFformula == vanilla_CFformula_with_mean_sims:
		CFformula_str = "vanilla_CFformula_with_mean_sims"


	print("CFformula = {}; k_min = {}; k_max = {}; by = {}; drop_percentile = {}; timestamp_mins = {}; traintest_seed = {}; mean_centering = {}; ttsplit_per_user = {}".format(\
			CFformula_str, str(k_min), str(k_max), str(by), str(drop_percentile), str(timestamp_mins), str(traintest_seed), str(mean_centering), str(ttsplit_per_user)))


	#################################
	# 2. SPLIT TRAIN/TEST SET       #
	#################################

	print("Split ratings into training and testing set...")

	# make train/test split
	if ttsplit_per_user:
		train_frac = (n_splits-1)/n_splits
		train_df, test_df = make_train_test_split_per_user(rating_df, train_frac = train_frac, traintest_seed = traintest_seed)
	else:
		train_df, test_df = make_train_test_split(rating_df, n_splits = n_splits, traintest_seed = traintest_seed, shuffle = True, foldNr = foldNr)#

	####################################
	# 3. EVALUATE
	####################################

	# initialize the result pandas.DataFrame
	result_df = pd.DataFrame()

	# if mean_centering is True: calculate mean_dict : {userId: mean_rating}
	if mean_centering:
		mean_dict = make_mean_dict(train_df)
	else:
		mean_dict = None

	# for all snapshot_dirs (that is users that are respresented by their snapshot_file):
	for i, snapshot_dir in enumerate(snapshot_dirs,0):
		# EVALUATE on the fold and the snapshot_dir's snapshot_df's
		#############################################################
		print("Build predictions dictionary from {}...".format(snapshot_dir))
		# set timestamp_min, if any are given (either a list of timestamp_deltas or None)
		if timestamp_mins is None:
			timestamp_min = None
		else:
			timestamp_min = timestamp_mins[i]
		# make predictions dict
		# format:	userId: array([true_rating, predicted_rating])
		#	{'AJGU56YG8G1DQ': array([[4.        , nan],
        #                            [5.        , 3.77268979],
        #                            [5.        , 4.94773482]])}

		predictions_dict_with_nans		= make_predictions_dict_inparallel(test_df, snapshot_output_path = snapshot_dir, drop_nans = False, k_min = k_min,\
								CFformula = CFformula, k_max = k_max, by = by, drop_percentile = drop_percentile, timestamp_min = timestamp_min, mean_dict = mean_dict)

		predictions_dict_without_nans = remove_nan_rows_from_predictions_dict(predictions_dict_with_nans)

		if mean_dict is not None:
			baseline_predictions_dict_without_nans = make_baseline_predictions_dict(predictions_dict_without_nans, mean_dict)

		peers = list(predictions_dict_without_nans.keys())

		# ACCURACY
		##################
		# NOTE that the MSE is the average over all users' individual MSE values instead of the overall MSE (!)
		accuracy_dict, MSE	= get_accuracy(predictions_dict_without_nans)
		MSE_overall			= get_accuracy_overall(predictions_dict_without_nans)

		#if baseline_predictions_dict_without_nans is not None:
		#	baseline_accuracy_dict, baseline_MSE	= get_accuracy(baseline_predictions_dict_without_nans)
		#	baseline_MSE_overall					= get_accuracy_overall(baseline_predictions_dict_without_nans)

		#	print("baseline_MSE", baseline_MSE)
		#	print("baseline_MSE_overall", baseline_MSE_overall)


		# COVERAGE
		###################
		# overall coverage
		overall_coverage = get_overall_coverage(predictions_dict_with_nans)
		# per-user coverage
		per_user_coverage = get_per_user_coverage(predictions_dict_with_nans)
		# utility coverage
		utility_coverage = get_utility_coverage(predictions_dict_without_nans, k_items = 10, threshold = 3.5)
		# user-space coverage
		user_space_coverage = get_user_space_coverage(snapshot_dir, peers, k_items = 10, drop_percentile = drop_percentile, by = by, k_min = k_min)
		#

		# PRECISION/RECALL
		###################
		# get the precisions and recalls in a dictionary @K (for recommended lists of K items)
		precisions_dict, recalls_dict, F1_dict, mean_precision, mean_recall, mean_F1 = get_precision_and_recall(predictions_dict_without_nans, threshold = 3.5, K =10)

		# get the quotient to the RMSE and the utility coverage
		if utility_coverage != 0.0:
			RMSE_cov = math.sqrt(MSE)/utility_coverage
		else:
			RMSE_cov = np.nan

		# ASSEMBLE RESULT TABLE
		###########################
		snapshot_dict = unmake_snapshot_info_string(snapshot_dirs[i])
		row_index = experiment_name + ":" + "t=" + str(snapshot_dict["t"])

		result_row = pd.DataFrame([[MSE, MSE_overall, per_user_coverage, overall_coverage]], index = [row_index]).applymap(lambda x: round(x,round_i))
		result_df = result_df.append(result_row)
	result_df.columns = ["MSE(mean)", "MSE(overall)", "Cov.(mean)", "Cov.(overall)"]
	print(result_df)

	# LOG RESULT TABLE
	#############################
	log_result_table(result_df, experiment_name)

	return result_df












if __name__ == "__main__":
	# evaluation parameters
	#########################
	# default paths
	output_dir			= os.path.join(root_dir,"data/snapshots")
	sim_dict_pickle_dir = os.path.join(root_dir,"data/similarity_dicts")
	# custom paths
	dataset_path 		= os.path.join(root_dir,"data/ml-25m/samples/ml-25m:n=500:rseed=1234.csv")
	# default parameters
	foldNr				= 0


	##############
	# PARSE ARGS #
	##############
	parser = argparse.ArgumentParser()
	parser.add_argument("experiment_dir"	, help="directory which holds snapshot directories, that is directories with snapshot *.csv files per user" 		, action = "store")
	parser.add_argument("dataset_path"		, help="path to the ratings_df used for the experiment" 		, action = "store")
	parser.add_argument("CFformula"			, help="string that defines the CFformula to use for rating estimation: \
		'vanilla_CFformula', 'vanilla_CFformula_with_mean_sims', 'vanilla_CFformula_with_sim_to_sender', 'aggregated_CFformula', 'unweighted_CFformula'" 		, action = "store")
	parser.add_argument("--drop_nans", help="'1': nan values in the predicted ratings; DEFAULT: do not drop nans" 												, action = "store")
	parser.add_argument("--k_min", help="<int>; only derive an estimated rating when a least k_min neighbor ratings are available; DEFAULT: 1" 								, action = "store")
	parser.add_argument("--k_max", help="<int>; only derive an estimated rating on the k_max most similar neighbors' ratings. Here, neighbor might be the data subject (DecCf),\
						or the neighborhood representative, that is the sender (DecAggCF, DecShCF); DEFAULT: 1,000,000"		 									, action = "store")
	parser.add_argument("--by", help="column-name in the snapshot_df to use for limiting neighborhoods in rating estimation (see e.g. '--k_max');\
							 DEFAULT = 'sim_to_sender", action = "store")

	args = parser.parse_args()

	experiment_dir = args.experiment_dir
	dataset_path = args.dataset_path
	
	if args.CFformula == 'vanilla_CFformula':
		CFformula = vanilla_CFformula
	elif args.CFformula == 'vanilla_CFformula_with_mean_sims':
		CFformula = vanilla_CFformula_with_mean_sims
	elif args.CFformula == 'vanilla_CFformula_with_sim_to_sender':
		CFformula = vanilla_CFformula_with_sim_to_sender
	elif args.CFformula == 'aggregated_CFformula':
		CFformula = aggregated_CFformula
	elif args.CFformula == 'unweighted_CFformula':
		CFformula = unweighted_CFformula

	if args.drop_nans == "1":
		drop_nans = True
	else:
		drop_nans = False # default

	if args.k_min is not None:
		k_min = int(args.k_min)
	else:
		k_min = 1 # default

	if args.k_max is not None:
		k_max = int(args.k_max)
	else:
		k_max = 1000000 # default

	if args.by is not None:
		by = args.by
	else:
		by = "sim_to_sender" # default

	experiment_name = os.path.split(experiment_dir)[1]
	snapshot_dirs = [os.path.join(experiment_dir,el) for el in os.listdir(experiment_dir) if os.path.isdir(os.path.join(experiment_dir,el)) ]
	# sort snapshot_dirs in ascending order such that snapshots that have finished earlier also appear earlier in the evaluation output
	snapshot_dirs.sort()
	
	snapshot_info_strings = [os.path.split(el)[-1] for el in snapshot_dirs] # the names of directories in the experiment_dir
	# (!) here, we assume that the parameterization of all snapshots in the experiment_dir is identical
	snapshot_info_dict 	= unmake_snapshot_info_string(snapshot_info_strings[0])
	# experiment parameters
	n_splits 			= snapshot_info_dict["n_splits"]
	traintest_seed 		= snapshot_info_dict["traintest_seed"]
	foldNr				= 0 # default, see WHERE ?!


	run_evaluation(dataset_path, n_splits, traintest_seed, foldNr, snapshot_dirs, drop_nans = drop_nans,\
							CFformula = CFformula, k_min = k_min, k_max = k_max, by = by, experiment_name = experiment_name)
